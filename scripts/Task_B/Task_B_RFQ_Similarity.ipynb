{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdf460b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading RFQ data...\n",
      "RFQ data shape: (1000, 25)\n",
      "\n",
      "Loading reference properties...\n",
      "Reference data shape: (175, 34)\n",
      "\n",
      "Unique grades in RFQ: 158\n",
      "Missing grades in RFQ: 59\n",
      "\n",
      "Unique grades in Reference: 175\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def load_rfq_data():\n",
    "    \"\"\"Load and perform initial inspection of RFQ and reference data.\"\"\"\n",
    "    \n",
    "    # Define paths\n",
    "    task2_dir = Path(\"../../resources/task_2\")\n",
    "    rfq_path = task2_dir / \"rfq.csv\"\n",
    "    reference_path = task2_dir / \"reference_properties.tsv\"\n",
    "    \n",
    "    # Load datasets\n",
    "    print(\"Loading RFQ data...\")\n",
    "    rfq_df = pd.read_csv(rfq_path)\n",
    "    print(f\"RFQ data shape: {rfq_df.shape}\")\n",
    "    \n",
    "    print(\"\\nLoading reference properties...\")\n",
    "    reference_df = pd.read_csv(reference_path, sep='\\t')\n",
    "    print(f\"Reference data shape: {reference_df.shape}\")\n",
    "    \n",
    "    # Basic data inspection  \n",
    "    print(f\"\\nUnique grades in RFQ: {rfq_df['grade'].nunique()}\")\n",
    "    print(f\"Missing grades in RFQ: {rfq_df['grade'].isnull().sum()}\")\n",
    "    \n",
    "    print(f\"\\nUnique grades in Reference: {reference_df['Grade/Material'].nunique()}\")\n",
    "    \n",
    "    return rfq_df, reference_df\n",
    "\n",
    "# Execute the loading function\n",
    "if __name__ == \"__main__\":\n",
    "    rfq_df, reference_df = load_rfq_data()\n",
    "    \n",
    "    # Display first few rows to understand structure\n",
    "    # print(\"\\nFirst 3 RFQ rows:\")\n",
    "    # print(rfq_df.head(3))\n",
    "    \n",
    "    # print(\"\\nFirst 3 Reference rows:\")\n",
    "    # print(reference_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "563e1a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TASK B.1: Grade Normalization and Reference Join ===\n",
      "\n",
      "Grade Analysis:\n",
      "Unique normalized grades in RFQ: 156\n",
      "Unique normalized grades in Reference: 173\n",
      "\n",
      "Grades found in both datasets: 156\n",
      "RFQ grades missing in reference: 0\n",
      "Reference grades not in RFQ: 17\n",
      "\n",
      "Successful grade mappings created: 156\n",
      "\n",
      "Join Results:\n",
      "Total RFQ records: 1000\n",
      "Records with reference data: 946\n",
      "Records missing reference data: 59\n",
      "\n",
      "Enriched dataset shape: (1005, 62)\n",
      "Enriched dataset has been saved as 'enriched_dataset.csv'\n"
     ]
    }
   ],
   "source": [
    "def normalize_grades(rfq_df, reference_df):\n",
    "    \"\"\"\n",
    "    Normalize grade keys and handle grade matching between RFQ and reference data.\n",
    "    \n",
    "    Task B.1: Normalize grade keys (case, suffixes, aliases) and join RFQs with reference.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== TASK B.1: Grade Normalization and Reference Join ===\")\n",
    "    \n",
    "    # Create working copies\n",
    "    rfq_work = rfq_df.copy()\n",
    "    ref_work = reference_df.copy()\n",
    "    \n",
    "    # Step 1: Normalize grade formats\n",
    "    def clean_grade(grade):\n",
    "        \"\"\"Clean and normalize grade strings.\"\"\"\n",
    "        if pd.isna(grade):\n",
    "            return None\n",
    "        \n",
    "        # Convert to string, strip whitespace, uppercase\n",
    "        grade = str(grade).strip().upper()\n",
    "        \n",
    "        # Remove common prefixes/suffixes that might cause mismatches\n",
    "        # Remove trailing numbers that might be variants (e.g., S235JR -> S235)\n",
    "        # But be careful not to remove essential numbers (e.g., keep S700MC as is)\n",
    "        \n",
    "        return grade\n",
    "    \n",
    "    # Apply grade normalization\n",
    "    rfq_work['grade_normalized'] = rfq_work['grade'].apply(clean_grade)\n",
    "    ref_work['grade_normalized'] = ref_work['Grade/Material'].apply(clean_grade)\n",
    "    \n",
    "    # Step 2: Analyze grade matching\n",
    "    print(f\"\\nGrade Analysis:\")\n",
    "    print(f\"Unique normalized grades in RFQ: {rfq_work['grade_normalized'].nunique()}\")\n",
    "    print(f\"Unique normalized grades in Reference: {ref_work['grade_normalized'].nunique()}\")\n",
    "    \n",
    "    # Find overlap and missing grades\n",
    "    rfq_grades = set(rfq_work['grade_normalized'].dropna())\n",
    "    ref_grades = set(ref_work['grade_normalized'])\n",
    "    \n",
    "    common_grades = rfq_grades.intersection(ref_grades)\n",
    "    rfq_missing_in_ref = rfq_grades - ref_grades\n",
    "    ref_not_in_rfq = ref_grades - rfq_grades\n",
    "    \n",
    "    print(f\"\\nGrades found in both datasets: {len(common_grades)}\")\n",
    "    print(f\"RFQ grades missing in reference: {len(rfq_missing_in_ref)}\")\n",
    "    print(f\"Reference grades not in RFQ: {len(ref_not_in_rfq)}\")\n",
    "    \n",
    "    if rfq_missing_in_ref:\n",
    "        print(f\"\\nSample RFQ grades missing in reference: {list(rfq_missing_in_ref)[:10]}\")\n",
    "    \n",
    "    # Step 3: Create grade mapping for better matching\n",
    "    # Handle common aliases and variants\n",
    "    def create_grade_mapping(rfq_grades, ref_grades):\n",
    "        \"\"\"Create mapping for grade aliases and variants.\"\"\"\n",
    "        mapping = {}\n",
    "        \n",
    "        # Direct matches first\n",
    "        for grade in rfq_grades:\n",
    "            if grade in ref_grades:\n",
    "                mapping[grade] = grade\n",
    "        \n",
    "        # Handle common aliases (you can expand this based on domain knowledge)\n",
    "        alias_rules = {\n",
    "            # Add specific mappings if you know them\n",
    "            # Example: 'S235' might map to 'S235JR'\n",
    "        }\n",
    "        \n",
    "        # Try to match truncated versions\n",
    "        for rfq_grade in rfq_grades:\n",
    "            if rfq_grade not in mapping:\n",
    "                # Try to find partial matches\n",
    "                for ref_grade in ref_grades:\n",
    "                    if rfq_grade in ref_grade or ref_grade in rfq_grade:\n",
    "                        # Be careful with partial matching - only if reasonable\n",
    "                        if len(rfq_grade) >= 4:  # Avoid matching very short strings\n",
    "                            mapping[rfq_grade] = ref_grade\n",
    "                            break\n",
    "        \n",
    "        return mapping\n",
    "    \n",
    "    grade_mapping = create_grade_mapping(rfq_grades, ref_grades)\n",
    "    print(f\"\\nSuccessful grade mappings created: {len(grade_mapping)}\")\n",
    "    \n",
    "    # Step 4: Apply mapping and join\n",
    "    rfq_work['grade_mapped'] = rfq_work['grade_normalized'].map(grade_mapping)\n",
    "    \n",
    "    # Join RFQ with reference data\n",
    "    enriched_rfq = rfq_work.merge(\n",
    "        ref_work, \n",
    "        left_on='grade_mapped', \n",
    "        right_on='grade_normalized',\n",
    "        how='left',\n",
    "        suffixes=('', '_ref')\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nJoin Results:\")\n",
    "    print(f\"Total RFQ records: {len(rfq_work)}\")\n",
    "    print(f\"Records with reference data: {enriched_rfq['Grade/Material'].notna().sum()}\")\n",
    "    print(f\"Records missing reference data: {enriched_rfq['Grade/Material'].isna().sum()}\")\n",
    "    \n",
    "    return enriched_rfq, grade_mapping\n",
    "\n",
    "# Execute the normalization and joining\n",
    "enriched_rfq, grade_mapping = normalize_grades(rfq_df, reference_df)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nEnriched dataset shape: {enriched_rfq.shape}\")\n",
    "# Save the enriched dataset to a CSV file\n",
    "enriched_rfq.to_csv('enriched_dataset.csv', index=False)\n",
    "print(\"Enriched dataset has been saved as 'enriched_dataset.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f736cfa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TASK B.1: Grade Normalization and Reference Join ===\n",
      "Reference data before deduplication: 175 rows\n",
      "Reference data after deduplication: 164 rows\n",
      "Removed 11 duplicate reference entries\n",
      "\n",
      "Grade Analysis:\n",
      "Unique normalized grades in RFQ: 147\n",
      "Unique normalized grades in Reference: 164\n",
      "\n",
      "Grades found in both datasets: 147\n",
      "RFQ grades missing in reference: 0\n",
      "Reference grades not in RFQ: 17\n",
      "\n",
      "Successful grade mappings created: 147\n",
      "\n",
      "Join Results:\n",
      "Original RFQ records: 1000\n",
      "Final enriched records: 1000\n",
      "Records with reference data: 941\n",
      "Records missing reference data: 59\n",
      "SUCCESS: No duplicate rows created!\n",
      "\n",
      "Final dataset shape: (1000, 61)\n",
      "\n",
      "Enriched dataset shape: (1000, 61)\n",
      "Fixed enriched dataset has been saved as 'enriched_dataset.csv'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from difflib import get_close_matches\n",
    "\n",
    "def normalize_grades(rfq_df, reference_df):\n",
    "    \"\"\"\n",
    "    Normalize grade keys and handle grade matching between RFQ and reference data.\n",
    "    \n",
    "    Task B.1: Normalize grade keys (case, suffixes, aliases) and join RFQs with reference.\n",
    "    FIXED: Eliminates duplicate rows and reduces redundant columns.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== TASK B.1: Grade Normalization and Reference Join ===\")\n",
    "    \n",
    "    # Create working copies\n",
    "    rfq_work = rfq_df.copy()\n",
    "    ref_work = reference_df.copy()\n",
    "    \n",
    "    # Step 1: Normalize grade formats\n",
    "    def clean_grade(grade):\n",
    "        \"\"\"Automatically clean and normalize grade strings.\"\"\"\n",
    "        if pd.isna(grade):\n",
    "            return None\n",
    "        \n",
    "        grade = str(grade).strip().upper()\n",
    "        \n",
    "        # Remove delivery condition suffixes (+N, +QT, +C)\n",
    "        grade = re.sub(r\"\\+.*$\", \"\", grade)\n",
    "        \n",
    "        # Remove spaces and dashes\n",
    "        grade = grade.replace(\" \", \"\").replace(\"-\", \"\")\n",
    "        \n",
    "        # Normalize Werkstoffnummer like \"1.2343\" (keep consistent format)\n",
    "        if re.match(r\"^\\d\\.\\d{3,4}$\", grade):\n",
    "            return grade\n",
    "        \n",
    "        # DX grades: add missing \"D\" at end if not present\n",
    "        if re.match(r\"^DX\\d{2}$\", grade):\n",
    "            grade = grade + \"D\"\n",
    "        \n",
    "        return grade\n",
    "    \n",
    "    # Apply grade normalization\n",
    "    rfq_work['grade_normalized'] = rfq_work['grade'].apply(clean_grade)\n",
    "    ref_work['grade_normalized'] = ref_work['Grade/Material'].apply(clean_grade)\n",
    "    \n",
    "    # Step 1.5: DEDUPLICATE REFERENCE DATA to prevent duplicate joins\n",
    "    print(f\"Reference data before deduplication: {len(ref_work)} rows\")\n",
    "    \n",
    "    def select_best_reference(group):\n",
    "        \"\"\"Select the best reference entry when multiple exist for same normalized grade.\"\"\"\n",
    "        if len(group) == 1:\n",
    "            return group.iloc[0]\n",
    "        \n",
    "        # Preference: exact match to normalized grade, then shortest original name\n",
    "        original_upper = group['Grade/Material'].str.upper().str.replace(' ', '').str.replace('-', '')\n",
    "        exact_matches = group[original_upper == group['grade_normalized']]\n",
    "        \n",
    "        if len(exact_matches) > 0:\n",
    "            return exact_matches.iloc[0]\n",
    "        \n",
    "        # Otherwise, pick shortest name (likely has fewer suffixes)\n",
    "        sorted_group = group.loc[group['Grade/Material'].str.len().idxmin()]\n",
    "        return sorted_group\n",
    "    \n",
    "    ref_deduplicated = ref_work.groupby('grade_normalized', group_keys=False).apply(select_best_reference).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Reference data after deduplication: {len(ref_deduplicated)} rows\")\n",
    "    print(f\"Removed {len(ref_work) - len(ref_deduplicated)} duplicate reference entries\")\n",
    "    \n",
    "    # Step 2: Analyze grade matching\n",
    "    print(f\"\\nGrade Analysis:\")\n",
    "    print(f\"Unique normalized grades in RFQ: {rfq_work['grade_normalized'].nunique()}\")\n",
    "    print(f\"Unique normalized grades in Reference: {ref_deduplicated['grade_normalized'].nunique()}\")\n",
    "    \n",
    "    rfq_grades = set(rfq_work['grade_normalized'].dropna())\n",
    "    ref_grades = set(ref_deduplicated['grade_normalized'].dropna())\n",
    "    \n",
    "    common_grades = rfq_grades.intersection(ref_grades)\n",
    "    rfq_missing_in_ref = rfq_grades - ref_grades\n",
    "    ref_not_in_rfq = ref_grades - rfq_grades\n",
    "    \n",
    "    print(f\"\\nGrades found in both datasets: {len(common_grades)}\")\n",
    "    print(f\"RFQ grades missing in reference: {len(rfq_missing_in_ref)}\")\n",
    "    print(f\"Reference grades not in RFQ: {len(ref_not_in_rfq)}\")\n",
    "    \n",
    "    if rfq_missing_in_ref:\n",
    "        print(f\"\\nSample RFQ grades missing in reference: {list(rfq_missing_in_ref)[:10]}\")\n",
    "    \n",
    "    # Step 3: Create grade mapping automatically\n",
    "    def create_grade_mapping(rfq_grades, ref_grades):\n",
    "        \"\"\"Auto-match RFQ grades to reference grades using fuzzy + substring matching.\"\"\"\n",
    "        mapping = {}\n",
    "        \n",
    "        for g in rfq_grades:\n",
    "            if g in ref_grades:\n",
    "                mapping[g] = g\n",
    "            else:\n",
    "                # Try fuzzy close matches\n",
    "                matches = get_close_matches(g, ref_grades, n=1, cutoff=0.8)\n",
    "                if matches:\n",
    "                    mapping[g] = matches[0]\n",
    "                else:\n",
    "                    # Try substring containment\n",
    "                    for ref_g in ref_grades:\n",
    "                        if g in ref_g or ref_g in g:\n",
    "                            mapping[g] = ref_g\n",
    "                            break\n",
    "        return mapping\n",
    "    \n",
    "    grade_mapping = create_grade_mapping(rfq_grades, ref_grades)\n",
    "    print(f\"\\nSuccessful grade mappings created: {len(grade_mapping)}\")\n",
    "    \n",
    "    # Step 4: Apply mapping and join (now guaranteed 1:1 mapping)\n",
    "    rfq_work['grade_mapped'] = rfq_work['grade_normalized'].map(grade_mapping)\n",
    "    \n",
    "    enriched_rfq = rfq_work.merge(\n",
    "        ref_deduplicated, \n",
    "        left_on='grade_mapped', \n",
    "        right_on='grade_normalized',\n",
    "        how='left',\n",
    "        suffixes=('', '_ref')\n",
    "    )\n",
    "    \n",
    "    # Step 5: Clean up redundant columns and add validation\n",
    "    original_count = len(rfq_work)\n",
    "    final_count = len(enriched_rfq)\n",
    "    \n",
    "    print(f\"\\nJoin Results:\")\n",
    "    print(f\"Original RFQ records: {original_count}\")\n",
    "    print(f\"Final enriched records: {final_count}\")\n",
    "    print(f\"Records with reference data: {enriched_rfq['Grade/Material'].notna().sum()}\")\n",
    "    print(f\"Records missing reference data: {enriched_rfq['Grade/Material'].isna().sum()}\")\n",
    "    \n",
    "    # Validation check\n",
    "    if final_count > original_count:\n",
    "        print(f\"WARNING: Still have {final_count - original_count} duplicate rows!\")\n",
    "        print(\"This shouldn't happen with the deduplication fix.\")\n",
    "    else:\n",
    "        print(\"SUCCESS: No duplicate rows created!\")\n",
    "    \n",
    "    # Clean up redundant columns to reduce confusion\n",
    "    # Keep only essential grade columns\n",
    "    columns_to_drop = ['grade_normalized_ref']  # This is redundant with grade_mapped\n",
    "    for col in columns_to_drop:\n",
    "        if col in enriched_rfq.columns:\n",
    "            enriched_rfq = enriched_rfq.drop(columns=[col])\n",
    "    \n",
    "    print(f\"\\nFinal dataset shape: {enriched_rfq.shape}\")\n",
    "    return enriched_rfq, grade_mapping\n",
    "\n",
    "# Execute the normalization and joining\n",
    "enriched_rfq, grade_mapping = normalize_grades(rfq_df, reference_df)\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nEnriched dataset shape: {enriched_rfq.shape}\")\n",
    "# Save the enriched dataset to a CSV file\n",
    "enriched_rfq.to_csv('enriched_dataset.csv', index=False)\n",
    "print(\"Fixed enriched dataset has been saved as 'enriched_dataset.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "90e226e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STEP 3: Range String Parsing ===\n",
      "Parsing Carbon (C)...\n",
      "  Successfully parsed 940/941 values\n",
      "Parsing Manganese (Mn)...\n",
      "  Successfully parsed 940/941 values\n",
      "Parsing Silicon (Si)...\n",
      "  Successfully parsed 539/540 values\n",
      "Parsing Sulfur (S)...\n",
      "  Successfully parsed 929/929 values\n",
      "Parsing Phosphorus (P)...\n",
      "  Successfully parsed 929/929 values\n",
      "Parsing Chromium (Cr)...\n",
      "  Successfully parsed 63/63 values\n",
      "Parsing Nickel (Ni)...\n",
      "  Successfully parsed 22/22 values\n",
      "Parsing Molybdenum (Mo)...\n",
      "  Successfully parsed 41/41 values\n",
      "Parsing Vanadium (V)...\n",
      "  Successfully parsed 227/227 values\n",
      "Parsing Copper (Cu)...\n",
      "  Successfully parsed 0/0 values\n",
      "Parsing Aluminum (Al)...\n",
      "  Successfully parsed 136/540 values\n",
      "Parsing Titanium (Ti)...\n",
      "  Successfully parsed 230/230 values\n",
      "Parsing Niobium (Nb)...\n",
      "  Successfully parsed 229/229 values\n",
      "Parsing Boron (B)...\n",
      "  Successfully parsed 7/7 values\n",
      "Parsing Nitrogen (N)...\n",
      "  Successfully parsed 136/136 values\n",
      "Parsing Tensile strength (Rm)...\n",
      "  Successfully parsed 940/941 values\n",
      "Parsing Yield strength (Re or Rp0.2)...\n",
      "  Successfully parsed 350/930 values\n",
      "Parsing Elongation (A%)...\n",
      "  Successfully parsed 2/921 values\n",
      "Total properties parsed: 18\n",
      "Final enriched dataset shape: (1000, 115)\n",
      "Fixed enriched_with_ranges dataset has been saved as 'enriched_with_ranges.csv'\n"
     ]
    }
   ],
   "source": [
    "# Parse Range Strings into Numeric Values\n",
    "\n",
    "def parse_range_strings(enriched_df):\n",
    "    \"\"\"\n",
    "    Parse range strings into numeric min/max values (and optionally mid).\n",
    "    \n",
    "    Handles formats like: '≤0.17', '360-510 MPa', '≥235 MPa', etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== STEP 3: Range String Parsing ===\")\n",
    "    \n",
    "    df_work = enriched_df.copy()\n",
    "    \n",
    "    def parse_range_value(value_str):\n",
    "        \"\"\"\n",
    "        Parse a range string and return (min_val, max_val, mid_val).\n",
    "        Returns (None, None, None) if unparseable.\n",
    "        \"\"\"\n",
    "        if pd.isna(value_str) or value_str == '':\n",
    "            return None, None, None\n",
    "        \n",
    "        # Convert to string and clean\n",
    "        value_str = str(value_str).strip()\n",
    "        \n",
    "        # Remove units (MPa, %, HB, HV, HRC, etc.)\n",
    "        clean_str = re.sub(r'[A-Za-z%°]', '', value_str)\n",
    "        clean_str = re.sub(r'\\s+', ' ', clean_str).strip()\n",
    "        \n",
    "        try:\n",
    "            # Pattern 1: ≤X or <=X (upper bound)\n",
    "            if '≤' in value_str or '<=' in value_str:\n",
    "                max_val = float(re.findall(r'[\\d.]+', clean_str)[0])\n",
    "                mid_val = max_val / 2 \n",
    "                return None, max_val, mid_val\n",
    "            \n",
    "            # Pattern 2: ≥X or >=X (lower bound)  \n",
    "            elif '≥' in value_str or '>=' in value_str:\n",
    "                min_val = float(re.findall(r'[\\d.]+', clean_str)[0])\n",
    "                return min_val, None, None\n",
    "            \n",
    "            # Pattern 3: X-Y or X–Y (range)\n",
    "            elif '-' in clean_str or '–' in clean_str:\n",
    "                numbers = re.findall(r'[\\d.]+', clean_str)\n",
    "                if len(numbers) >= 2:\n",
    "                    min_val = float(numbers[0])\n",
    "                    max_val = float(numbers[1])\n",
    "                    mid_val = (min_val + max_val) / 2\n",
    "                    return min_val, max_val, mid_val\n",
    "            \n",
    "            # Pattern 4: Single number\n",
    "            else:\n",
    "                numbers = re.findall(r'[\\d.]+', clean_str)\n",
    "                if numbers:\n",
    "                    val = float(numbers[0])\n",
    "                    return None, None, val\n",
    "            \n",
    "        except (ValueError, IndexError):\n",
    "            pass\n",
    "        \n",
    "        return None, None, None\n",
    "    \n",
    "    # Identify reference property columns that need parsing\n",
    "    chemical_props = ['Carbon (C)', 'Manganese (Mn)', 'Silicon (Si)', 'Sulfur (S)', \n",
    "                     'Phosphorus (P)', 'Chromium (Cr)', 'Nickel (Ni)', 'Molybdenum (Mo)',\n",
    "                     'Vanadium (V)', 'Copper (Cu)', 'Aluminum (Al)', 'Titanium (Ti)',\n",
    "                     'Niobium (Nb)', 'Boron (B)', 'Nitrogen (N)']\n",
    "    \n",
    "    mechanical_props = ['Tensile strength (Rm)', 'Yield strength (Re or Rp0.2)', 'Elongation (A%)']\n",
    "    \n",
    "    properties_to_parse = chemical_props + mechanical_props\n",
    "    \n",
    "    # Parse each property column\n",
    "    parsed_count = 0\n",
    "    for prop in properties_to_parse:\n",
    "        if prop in df_work.columns:\n",
    "            print(f\"Parsing {prop}...\")\n",
    "            \n",
    "            # Create new columns for min, max, mid values\n",
    "            prop_clean = prop.replace('(', '').replace(')', '').replace(' ', '_').replace('/', '_')\n",
    "            min_col = f\"{prop_clean}_min\"\n",
    "            max_col = f\"{prop_clean}_max\" \n",
    "            mid_col = f\"{prop_clean}_mid\"\n",
    "            \n",
    "            # Apply parsing\n",
    "            parsed_values = df_work[prop].apply(parse_range_value)\n",
    "            \n",
    "            df_work[min_col] = [x[0] for x in parsed_values]\n",
    "            df_work[max_col] = [x[1] for x in parsed_values]\n",
    "            df_work[mid_col] = [x[2] for x in parsed_values]\n",
    "            \n",
    "            # Count successful parses\n",
    "            non_null_count = df_work[mid_col].notna().sum()\n",
    "            total_non_null = df_work[prop].notna().sum()\n",
    "            \n",
    "            print(f\"  Successfully parsed {non_null_count}/{total_non_null} values\")\n",
    "            parsed_count += 1\n",
    "    \n",
    "    print(f\"Total properties parsed: {parsed_count}\")\n",
    "    \n",
    "    return df_work\n",
    "\n",
    "# Execute range parsing\n",
    "enriched_with_ranges = parse_range_strings(enriched_rfq)\n",
    "\n",
    "print(f\"Final enriched dataset shape: {enriched_with_ranges.shape}\")\n",
    "# Save the enriched_with_ranges dataset to a CSV file\n",
    "enriched_with_ranges.to_csv('enriched_with_ranges.csv', index=False)\n",
    "print(\"Fixed enriched_with_ranges dataset has been saved as 'enriched_with_ranges.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "76bf2a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TASK B.2: Feature Engineering ===\n",
      "\n",
      "1. Engineering Dimension Features...\n",
      "  Created interval features for thickness\n",
      "  Created interval features for width\n",
      "  Created interval features for length\n",
      "  Created interval features for height\n",
      "  Created interval features for weight\n",
      "  Created interval features for inner_diameter\n",
      "  Created interval features for outer_diameter\n",
      "  Created interval features for yield_strength\n",
      "  Created interval features for tensile_strength\n",
      "\n",
      "2. Engineering Categorical Features...\n",
      "  Standardized coating: 63 unique values\n",
      "  Standardized finish: 47 unique values\n",
      "  Standardized form: 18 unique values\n",
      "  Standardized surface_type: 15 unique values\n",
      "  Standardized surface_protection: 16 unique values\n",
      "\n",
      "3. Engineering Grade Property Features...\n",
      "\n",
      "  Grade Properties Availability and Filtering (min 5% coverage):\n",
      "    Carbon_C_mid: 940/941 (99.9%) kept\n",
      "    Manganese_Mn_mid: 940/941 (99.9%) kept\n",
      "    Silicon_Si_mid: 539/941 (57.3%) kept\n",
      "    Sulfur_S_mid: 929/941 (98.7%) kept\n",
      "    Phosphorus_P_mid: 929/941 (98.7%) kept\n",
      "    Chromium_Cr_mid: 63/941 (6.7%) kept\n",
      "    Nickel_Ni_mid: 22/941 (2.3%) dropped\n",
      "    Molybdenum_Mo_mid: 41/941 (4.4%) dropped\n",
      "    Vanadium_V_mid: 227/941 (24.1%) kept\n",
      "    Aluminum_Al_mid: 136/941 (14.5%) kept\n",
      "    Titanium_Ti_mid: 230/941 (24.4%) kept\n",
      "    Niobium_Nb_mid: 229/941 (24.3%) kept\n",
      "    Boron_B_mid: 7/941 (0.7%) dropped\n",
      "    Nitrogen_N_mid: 136/941 (14.5%) kept\n",
      "    Tensile_strength_Rm_mid: 940/941 (99.9%) kept\n",
      "    Yield_strength_Re_or_Rp0.2_mid: 350/941 (37.2%) kept\n",
      "    Elongation_A%_mid: 2/941 (0.2%) dropped\n",
      "\n",
      "4. Defining Overlap Metrics...\n",
      "  ✓ Interval overlap ratio function defined\n",
      "  ✓ Categorical match function defined\n",
      "\n",
      "=== Feature Engineering Summary ===\n",
      "Final dataset shape: (1000, 152)\n",
      "Interval features created: 36\n",
      "Categorical features standardized: 5\n",
      "Property midpoint features available: 14\n",
      "\n",
      "Fixed feature_engineered_df dataset has been saved as 'feature_engineered_df.csv'\n"
     ]
    }
   ],
   "source": [
    "def engineer_similarity_features(enriched_df):\n",
    "    \"\"\"\n",
    "    Create engineered features for similarity calculation.\n",
    "    \n",
    "    Task B.2: \n",
    "    - Dimensions: Represent as intervals, suggest overlap metrics\n",
    "    - Categorical: Define exact match (1/0) for coating, finish, form, surface_type\n",
    "    - Grade properties: Use numeric midpoints of ranges; drop very sparse features (<5% coverage)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== TASK B.2: Feature Engineering ===\")\n",
    "    \n",
    "    df_work = enriched_df.copy()\n",
    "    \n",
    "    # ========================================\n",
    "    # 1. DIMENSION FEATURES (Interval-based)\n",
    "    # ========================================\n",
    "    print(\"\\n1. Engineering Dimension Features...\")\n",
    "    \n",
    "    dimension_pairs = [\n",
    "        ('thickness_min', 'thickness_max'),\n",
    "        ('width_min', 'width_max'),\n",
    "        ('length_min', 'length_min'),  # Note: length only has min in data\n",
    "        ('height_min', 'height_max'),\n",
    "        ('weight_min', 'weight_max'),\n",
    "        ('inner_diameter_min', 'inner_diameter_max'),\n",
    "        ('outer_diameter_min', 'outer_diameter_max'),\n",
    "        ('yield_strength_min', 'yield_strength_max'),\n",
    "        ('tensile_strength_min', 'tensile_strength_max')\n",
    "    ]\n",
    "    \n",
    "    def create_interval_features(df, min_col, max_col, feature_name):\n",
    "        \"\"\"Create interval representation for dimensions.\"\"\"\n",
    "        df[f\"{feature_name}_interval_min\"] = df[min_col]\n",
    "        df[f\"{feature_name}_interval_max\"] = df[max_col].fillna(df[min_col])\n",
    "        df[f\"{feature_name}_center\"] = (df[f\"{feature_name}_interval_min\"] + df[f\"{feature_name}_interval_max\"]) / 2\n",
    "        df[f\"{feature_name}_width\"] = df[f\"{feature_name}_interval_max\"] - df[f\"{feature_name}_interval_min\"]\n",
    "        return df\n",
    "    \n",
    "    for min_col, max_col in dimension_pairs:\n",
    "        if min_col in df_work.columns and max_col in df_work.columns:\n",
    "            feature_name = min_col.replace('_min', '').replace('_max', '')\n",
    "            df_work = create_interval_features(df_work, min_col, max_col, feature_name)\n",
    "            print(f\"  Created interval features for {feature_name}\")\n",
    "    \n",
    "    # ========================================\n",
    "    # 2. CATEGORICAL FEATURES (Exact Match)\n",
    "    # ========================================\n",
    "    print(\"\\n2. Engineering Categorical Features...\")\n",
    "    \n",
    "    categorical_features = ['coating', 'finish', 'form', 'surface_type', 'surface_protection']\n",
    "    \n",
    "    for cat_feature in categorical_features:\n",
    "        if cat_feature in df_work.columns:\n",
    "            df_work[f\"{cat_feature}_clean\"] = df_work[cat_feature].fillna('Unknown').str.strip().str.upper()\n",
    "            print(f\"  Standardized {cat_feature}: {df_work[f'{cat_feature}_clean'].nunique()} unique values\")\n",
    "    \n",
    "    # ========================================\n",
    "    # 3. GRADE PROPERTY FEATURES (Midpoints) with sparsity filter\n",
    "    # ========================================\n",
    "    print(\"\\n3. Engineering Grade Property Features...\")\n",
    "    \n",
    "    chemical_features = [\n",
    "        'Carbon_C_mid', 'Manganese_Mn_mid', 'Silicon_Si_mid', \n",
    "        'Sulfur_S_mid', 'Phosphorus_P_mid', 'Chromium_Cr_mid',\n",
    "        'Nickel_Ni_mid', 'Molybdenum_Mo_mid', 'Vanadium_V_mid',\n",
    "        'Aluminum_Al_mid', 'Titanium_Ti_mid', 'Niobium_Nb_mid',\n",
    "        'Boron_B_mid', 'Nitrogen_N_mid'\n",
    "    ]\n",
    "    \n",
    "    mechanical_features = [\n",
    "        'Tensile_strength_Rm_mid', 'Yield_strength_Re_or_Rp0.2_mid', 'Elongation_A%_mid'\n",
    "    ]\n",
    "    \n",
    "    grade_features = chemical_features + mechanical_features\n",
    "    min_coverage = 5  # minimum % of rows required to keep a feature\n",
    "    kept_features = []\n",
    "    \n",
    "    print(\"\\n  Grade Properties Availability and Filtering (min 5% coverage):\")\n",
    "    for feature in grade_features:\n",
    "        if feature in df_work.columns:\n",
    "            non_null_count = df_work[feature].notna().sum()\n",
    "            total_with_ref = df_work['Grade/Material'].notna().sum()\n",
    "            coverage = non_null_count / total_with_ref * 100 if total_with_ref > 0 else 0\n",
    "            if coverage >= min_coverage:\n",
    "                kept_features.append(feature)\n",
    "            else:\n",
    "                df_work.drop(columns=[feature], inplace=True)\n",
    "            print(f\"    {feature}: {non_null_count}/{total_with_ref} ({coverage:.1f}%) {'kept' if coverage >= min_coverage else 'dropped'}\")\n",
    "    \n",
    "    # ========================================\n",
    "    # 4. OVERLAP METRIC FUNCTIONS\n",
    "    # ========================================\n",
    "    print(\"\\n4. Defining Overlap Metrics...\")\n",
    "    \n",
    "    def interval_overlap_ratio(min1, max1, min2, max2):\n",
    "        if pd.isna(min1) or pd.isna(max1) or pd.isna(min2) or pd.isna(max2):\n",
    "            return 0.0\n",
    "        min1, max1 = min(min1, max1), max(min1, max1)\n",
    "        min2, max2 = min(min2, max2), max(min2, max2)\n",
    "        overlap = max(0, min(max1, max2) - max(min1, min2))\n",
    "        union = max(max1, max2) - min(min1, min2)\n",
    "        return overlap / union if union > 0 else 0.0\n",
    "    \n",
    "    def categorical_match(val1, val2):\n",
    "        if pd.isna(val1) or pd.isna(val2):\n",
    "            return 0.0\n",
    "        return 1.0 if val1 == val2 else 0.0\n",
    "    \n",
    "    df_work.attrs['interval_overlap_ratio'] = interval_overlap_ratio\n",
    "    df_work.attrs['categorical_match'] = categorical_match\n",
    "    \n",
    "    print(\"  ✓ Interval overlap ratio function defined\")\n",
    "    print(\"  ✓ Categorical match function defined\")\n",
    "    \n",
    "    # ========================================\n",
    "    # 5. SUMMARY OF ENGINEERED FEATURES\n",
    "    # ========================================\n",
    "    print(f\"\\n=== Feature Engineering Summary ===\")\n",
    "    print(f\"Final dataset shape: {df_work.shape}\")\n",
    "    \n",
    "    interval_features = [col for col in df_work.columns if '_interval_' in col or '_center' in col or '_width' in col]\n",
    "    categorical_clean = [col for col in df_work.columns if '_clean' in col]\n",
    "    property_features = [col for col in df_work.columns if '_mid' in col]\n",
    "    \n",
    "    print(f\"Interval features created: {len(interval_features)}\")\n",
    "    print(f\"Categorical features standardized: {len(categorical_clean)}\")\n",
    "    print(f\"Property midpoint features available: {len(property_features)}\")\n",
    "    \n",
    "    return df_work\n",
    "\n",
    "# Execute feature engineering\n",
    "feature_engineered_df = engineer_similarity_features(enriched_with_ranges)\n",
    "\n",
    "# Save to CSV\n",
    "feature_engineered_df.to_csv('feature_engineered_df.csv', index=False)\n",
    "print(\"\\nFixed feature_engineered_df dataset has been saved as 'feature_engineered_df.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea9318ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting similarity calculation...\n",
      "=== TASK B.3: Similarity Calculation ===\n",
      "Feature groups defined:\n",
      "  Dimensions: 5 features (weight: 0.4)\n",
      "  Categorical: 4 features (weight: 0.3)\n",
      "  Grade properties: 7 features (weight: 0.3)\n",
      "\n",
      "Calculating pairwise similarities for 1000 RFQs...\n",
      "Valid RFQs for similarity calculation: 1000\n",
      "Computing similarities...\n",
      "  Progress: 5.0% (50/1000 RFQs processed)\n",
      "  Progress: 10.0% (100/1000 RFQs processed)\n",
      "  Progress: 15.0% (150/1000 RFQs processed)\n",
      "  Progress: 20.0% (200/1000 RFQs processed)\n",
      "  Progress: 25.0% (250/1000 RFQs processed)\n",
      "  Progress: 30.0% (300/1000 RFQs processed)\n",
      "  Progress: 35.0% (350/1000 RFQs processed)\n",
      "  Progress: 40.0% (400/1000 RFQs processed)\n",
      "  Progress: 45.0% (450/1000 RFQs processed)\n",
      "  Progress: 50.0% (500/1000 RFQs processed)\n",
      "  Progress: 55.0% (550/1000 RFQs processed)\n",
      "  Progress: 60.0% (600/1000 RFQs processed)\n",
      "  Progress: 65.0% (650/1000 RFQs processed)\n",
      "  Progress: 70.0% (700/1000 RFQs processed)\n",
      "  Progress: 75.0% (750/1000 RFQs processed)\n",
      "  Progress: 80.0% (800/1000 RFQs processed)\n",
      "  Progress: 85.0% (850/1000 RFQs processed)\n",
      "  Progress: 90.0% (900/1000 RFQs processed)\n",
      "  Progress: 95.0% (950/1000 RFQs processed)\n",
      "  Progress: 100.0% (1000/1000 RFQs processed)\n",
      "✓ Completed similarity calculations\n",
      "\n",
      "Similarity results shape: (3000, 6)\n",
      "Average similarity score: 0.579\n",
      "Max similarity score: 0.837\n",
      "\n",
      "Top 10 highest similarity pairs:\n",
      "                                    rfq_id  \\\n",
      "297   f19fd194-e55d-47b4-be13-fe00f147ebfb   \n",
      "300   86947c4f-30f3-44b2-bf27-b38f774d454a   \n",
      "294   e2cf8e2c-68d4-4334-9b7d-cc11fa0213dd   \n",
      "1926  03b99bdc-990c-4e85-af10-d9fd5ffd2a54   \n",
      "298   f19fd194-e55d-47b4-be13-fe00f147ebfb   \n",
      "1927  03b99bdc-990c-4e85-af10-d9fd5ffd2a54   \n",
      "295   e2cf8e2c-68d4-4334-9b7d-cc11fa0213dd   \n",
      "301   86947c4f-30f3-44b2-bf27-b38f774d454a   \n",
      "302   86947c4f-30f3-44b2-bf27-b38f774d454a   \n",
      "1928  03b99bdc-990c-4e85-af10-d9fd5ffd2a54   \n",
      "\n",
      "                                  match_id  similarity_score  \\\n",
      "297   86947c4f-30f3-44b2-bf27-b38f774d454a          0.836692   \n",
      "300   f19fd194-e55d-47b4-be13-fe00f147ebfb          0.836692   \n",
      "294   03b99bdc-990c-4e85-af10-d9fd5ffd2a54          0.835687   \n",
      "1926  e2cf8e2c-68d4-4334-9b7d-cc11fa0213dd          0.835687   \n",
      "298   03b99bdc-990c-4e85-af10-d9fd5ffd2a54          0.829283   \n",
      "1927  f19fd194-e55d-47b4-be13-fe00f147ebfb          0.829283   \n",
      "295   86947c4f-30f3-44b2-bf27-b38f774d454a          0.828609   \n",
      "301   e2cf8e2c-68d4-4334-9b7d-cc11fa0213dd          0.828609   \n",
      "302   03b99bdc-990c-4e85-af10-d9fd5ffd2a54          0.825975   \n",
      "1928  86947c4f-30f3-44b2-bf27-b38f774d454a          0.825975   \n",
      "\n",
      "      dimension_similarity  categorical_similarity  property_similarity  \n",
      "297                    0.6                     1.0             0.988975  \n",
      "300                    0.6                     1.0             0.988975  \n",
      "294                    0.6                     1.0             0.985625  \n",
      "1926                   0.6                     1.0             0.985625  \n",
      "298                    0.6                     1.0             0.964276  \n",
      "1927                   0.6                     1.0             0.964276  \n",
      "295                    0.6                     1.0             0.962032  \n",
      "301                    0.6                     1.0             0.962032  \n",
      "302                    0.6                     1.0             0.953251  \n",
      "1928                   0.6                     1.0             0.953251  \n"
     ]
    }
   ],
   "source": [
    "def calculate_rfq_similarity(df):\n",
    "    \"\"\"\n",
    "    Calculate aggregate similarity scores between RFQs.\n",
    "    \n",
    "    Task B.3: Define aggregate similarity score and output top-3 most similar RFQs per line.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== TASK B.3: Similarity Calculation ===\")\n",
    "    \n",
    "    # Get overlap functions from dataframe attributes\n",
    "    interval_overlap_ratio = df.attrs['interval_overlap_ratio']\n",
    "    categorical_match = df.attrs['categorical_match']\n",
    "    \n",
    "    # ========================================\n",
    "    # 1. DEFINE FEATURE GROUPS AND WEIGHTS\n",
    "    # ========================================\n",
    "    \n",
    "    # Dimension features (interval-based)\n",
    "    dimension_features = [\n",
    "        ('thickness_interval_min', 'thickness_interval_max'),\n",
    "        ('width_interval_min', 'width_interval_max'),\n",
    "        ('weight_interval_min', 'weight_interval_max'),\n",
    "        ('yield_strength_interval_min', 'yield_strength_interval_max'),\n",
    "        ('tensile_strength_interval_min', 'tensile_strength_interval_max')\n",
    "    ]\n",
    "    \n",
    "    # Categorical features (exact match)\n",
    "    categorical_features = ['coating_clean', 'finish_clean', 'form_clean', 'surface_type_clean']\n",
    "    \n",
    "    # Grade property features (numeric - use dense features only)\n",
    "    grade_property_features = [\n",
    "        'Carbon_C_mid', 'Manganese_Mn_mid', 'Sulfur_S_mid', \n",
    "        'Phosphorus_P_mid', 'Tensile_strength_Rm_mid', \n",
    "        'Yield_strength_Re_or_Rp0.2_mid', 'Elongation_A%_mid'\n",
    "    ]\n",
    "    \n",
    "    # Feature weights (can be tuned)\n",
    "    weights = {\n",
    "        'dimensions': 0.3,      # 30% weight for dimensional similarity\n",
    "        'categorical': 0.3,     # 30% weight for categorical matches  \n",
    "        'grade_properties': 0.4 # 40% weight for grade property similarity\n",
    "    }\n",
    "    \n",
    "    print(f\"Feature groups defined:\")\n",
    "    print(f\"  Dimensions: {len(dimension_features)} features (weight: {weights['dimensions']})\")\n",
    "    print(f\"  Categorical: {len(categorical_features)} features (weight: {weights['categorical']})\")\n",
    "    print(f\"  Grade properties: {len(grade_property_features)} features (weight: {weights['grade_properties']})\")\n",
    "    \n",
    "    # ========================================\n",
    "    # 2. SIMILARITY CALCULATION FUNCTIONS\n",
    "    # ========================================\n",
    "    \n",
    "    def calculate_dimension_similarity(row1, row2):\n",
    "        \"\"\"Calculate dimensional similarity using interval overlap.\"\"\"\n",
    "        similarities = []\n",
    "        \n",
    "        for min_col, max_col in dimension_features:\n",
    "            if min_col in df.columns and max_col in df.columns:\n",
    "                overlap = interval_overlap_ratio(\n",
    "                    row1[min_col], row1[max_col],\n",
    "                    row2[min_col], row2[max_col]\n",
    "                )\n",
    "                similarities.append(overlap)\n",
    "        \n",
    "        return np.mean(similarities) if similarities else 0.0\n",
    "    \n",
    "    def calculate_categorical_similarity(row1, row2):\n",
    "        \"\"\"Calculate categorical similarity using exact matches.\"\"\"\n",
    "        matches = []\n",
    "        \n",
    "        for cat_feature in categorical_features:\n",
    "            if cat_feature in df.columns:\n",
    "                match = categorical_match(row1[cat_feature], row2[cat_feature])\n",
    "                matches.append(match)\n",
    "        \n",
    "        return np.mean(matches) if matches else 0.0\n",
    "    \n",
    "    def calculate_grade_property_similarity(row1, row2):\n",
    "        \"\"\"Calculate grade property similarity using normalized differences.\"\"\"\n",
    "        similarities = []\n",
    "        \n",
    "        for prop_feature in grade_property_features:\n",
    "            if prop_feature in df.columns:\n",
    "                val1, val2 = row1[prop_feature], row2[prop_feature]\n",
    "                \n",
    "                if pd.notna(val1) and pd.notna(val2):\n",
    "                    # Normalize by the range of the feature to get 0-1 similarity\n",
    "                    feature_range = df[prop_feature].max() - df[prop_feature].min()\n",
    "                    if feature_range > 0:\n",
    "                        normalized_diff = abs(val1 - val2) / feature_range\n",
    "                        similarity = max(0, 1 - normalized_diff)  # Convert difference to similarity\n",
    "                        similarities.append(similarity)\n",
    "        \n",
    "        return np.mean(similarities) if similarities else 0.0\n",
    "    \n",
    "    def calculate_aggregate_similarity(row1, row2):\n",
    "        \"\"\"Calculate weighted aggregate similarity score.\"\"\"\n",
    "        dim_sim = calculate_dimension_similarity(row1, row2)\n",
    "        cat_sim = calculate_categorical_similarity(row1, row2)\n",
    "        prop_sim = calculate_grade_property_similarity(row1, row2)\n",
    "        \n",
    "        aggregate_score = (\n",
    "            weights['dimensions'] * dim_sim +\n",
    "            weights['categorical'] * cat_sim +\n",
    "            weights['grade_properties'] * prop_sim\n",
    "        )\n",
    "        \n",
    "        return aggregate_score, dim_sim, cat_sim, prop_sim\n",
    "    \n",
    "    # ========================================\n",
    "    # 3. CALCULATE PAIRWISE SIMILARITIES\n",
    "    # ========================================\n",
    "    \n",
    "    print(f\"\\nCalculating pairwise similarities for {len(df)} RFQs...\")\n",
    "    \n",
    "    # Filter to only RFQs with IDs (exclude NaN rows from join)\n",
    "    valid_df = df[df['id'].notna()].copy().reset_index(drop=True)\n",
    "    print(f\"Valid RFQs for similarity calculation: {len(valid_df)}\")\n",
    "    \n",
    "    # Initialize results list\n",
    "    similarity_results = []\n",
    "    \n",
    "    # Calculate similarities (this might take a moment for 1000x1000 comparisons)\n",
    "    print(\"Computing similarities...\")\n",
    "    \n",
    "    batch_size = 50  # Process in batches to show progress\n",
    "    total_comparisons = len(valid_df) * (len(valid_df) - 1)\n",
    "    completed = 0\n",
    "    \n",
    "    for i in range(0, len(valid_df), batch_size):\n",
    "        end_i = min(i + batch_size, len(valid_df))\n",
    "        \n",
    "        for idx1 in range(i, end_i):\n",
    "            row1 = valid_df.iloc[idx1]\n",
    "            rfq1_id = row1['id']\n",
    "            \n",
    "            # Store similarities for this RFQ\n",
    "            rfq_similarities = []\n",
    "            \n",
    "            for idx2 in range(len(valid_df)):\n",
    "                if idx1 != idx2:  # Exclude self-comparison\n",
    "                    row2 = valid_df.iloc[idx2]\n",
    "                    rfq2_id = row2['id']\n",
    "                    \n",
    "                    # Skip exact matching (same grade, same dimensions, etc.)\n",
    "                    if (row1['grade'] == row2['grade'] and \n",
    "                        row1['thickness_center'] == row2['thickness_center'] and\n",
    "                        row1['width_center'] == row2['width_center']):\n",
    "                        continue\n",
    "                    \n",
    "                    agg_sim, dim_sim, cat_sim, prop_sim = calculate_aggregate_similarity(row1, row2)\n",
    "                    \n",
    "                    rfq_similarities.append({\n",
    "                        'rfq_id': rfq1_id,\n",
    "                        'match_id': rfq2_id,\n",
    "                        'similarity_score': agg_sim,\n",
    "                        'dimension_similarity': dim_sim,\n",
    "                        'categorical_similarity': cat_sim,\n",
    "                        'property_similarity': prop_sim\n",
    "                    })\n",
    "            \n",
    "            # Get top-3 most similar for this RFQ\n",
    "            if rfq_similarities:\n",
    "                rfq_similarities.sort(key=lambda x: x['similarity_score'], reverse=True)\n",
    "                top3 = rfq_similarities[:3]\n",
    "                similarity_results.extend(top3)\n",
    "            \n",
    "            completed += len(valid_df) - 1\n",
    "        \n",
    "        # Progress update\n",
    "        progress = (end_i / len(valid_df)) * 100\n",
    "        print(f\"  Progress: {progress:.1f}% ({end_i}/{len(valid_df)} RFQs processed)\")\n",
    "    \n",
    "    print(f\"✓ Completed similarity calculations\")\n",
    "    \n",
    "    # ========================================\n",
    "    # 4. CREATE RESULTS DATAFRAME  \n",
    "    # ========================================\n",
    "    \n",
    "    results_df = pd.DataFrame(similarity_results)\n",
    "    print(f\"\\nSimilarity results shape: {results_df.shape}\")\n",
    "    print(f\"Average similarity score: {results_df['similarity_score'].mean():.3f}\")\n",
    "    print(f\"Max similarity score: {results_df['similarity_score'].max():.3f}\")\n",
    "    \n",
    "    # Show sample results\n",
    "    print(f\"\\nTop 10 highest similarity pairs:\")\n",
    "    top_results = results_df.nlargest(10, 'similarity_score')\n",
    "    print(top_results[['rfq_id', 'match_id', 'similarity_score', 'dimension_similarity', \n",
    "                      'categorical_similarity', 'property_similarity']])\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Execute similarity calculation\n",
    "print(\"Starting similarity calculation...\")\n",
    "similarity_results = calculate_rfq_similarity(feature_engineered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "848fdb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Creating Final Deliverable (top3.csv) ===\n",
      "\n",
      "✓ Deliverable saved to: ../../results/top3.csv\n",
      "✓ File contains 3000 similarity pairs\n",
      "\n",
      "=== FINAL SUMMARY REPORT ===\n",
      "Task B: RFQ Similarity Analysis - COMPLETED\n",
      "==================================================\n",
      "\n",
      "📊 DATA PROCESSING SUMMARY:\n",
      "   • Original RFQ records: 1,000\n",
      "   • RFQs with reference data: 941\n",
      "   • Grade properties parsed: 18 properties\n",
      "   • Feature engineering: 157 total features\n",
      "\n",
      "🔍 SIMILARITY ANALYSIS SUMMARY:\n",
      "   • Total similarity pairs calculated: 3,000\n",
      "   • Average similarity score: 0.579\n",
      "   • Similarity score range: 0.225 - 0.837\n",
      "\n",
      "⚖️ SIMILARITY COMPONENTS:\n",
      "   • Dimension similarity avg: 0.068\n",
      "   • Categorical similarity avg: 0.929\n",
      "   • Property similarity avg: 0.911\n",
      "\n",
      "🏆 TOP SIMILARITY PAIRS:\n",
      "   • f19fd194... ↔ 86947c4f... (score: 0.837)\n",
      "   • 86947c4f... ↔ f19fd194... (score: 0.837)\n",
      "   • e2cf8e2c... ↔ 03b99bdc... (score: 0.836)\n",
      "   • 03b99bdc... ↔ e2cf8e2c... (score: 0.836)\n",
      "   • f19fd194... ↔ 03b99bdc... (score: 0.829)\n",
      "\n",
      "🎉 TASK B COMPLETED SUCCESSFULLY!\n",
      "   All required deliverables have been generated.\n",
      "   The similarity pipeline is ready for production use.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Create Final Deliverable and Pipeline\n",
    "def create_final_deliverable(similarity_results):\n",
    "    \"\"\"\n",
    "    Create the final top3.csv deliverable as specified in Task B.3.\n",
    "    Format: [rfq_id, match_id, similarity_score]\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=== Creating Final Deliverable (top3.csv) ===\")\n",
    "    \n",
    "    # Select only the required columns\n",
    "    deliverable_df = similarity_results[['rfq_id', 'match_id', 'similarity_score']].copy()\n",
    "    \n",
    "    # Round similarity scores to 6 decimal places for clean output\n",
    "    deliverable_df['similarity_score'] = deliverable_df['similarity_score'].round(6)\n",
    "    \n",
    "    # Sort by rfq_id first, then by similarity_score (highest first) for better organization\n",
    "    deliverable_df = deliverable_df.sort_values(['rfq_id', 'similarity_score'], \n",
    "                                               ascending=[True, False])\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_path = Path(\"../../results/top3.csv\")\n",
    "    output_path.parent.mkdir(exist_ok=True)\n",
    "    \n",
    "    deliverable_df.to_csv(output_path, index=False)\n",
    "    print(f\"\\n✓ Deliverable saved to: {output_path}\")\n",
    "    print(f\"✓ File contains {len(deliverable_df)} similarity pairs\")\n",
    "    \n",
    "    return deliverable_df\n",
    "\n",
    "def create_summary_report(similarity_results, enriched_df):\n",
    "    \"\"\"Create a summary report of the RFQ similarity analysis.\"\"\"\n",
    "    \n",
    "    print(f\"\\n=== FINAL SUMMARY REPORT ===\")\n",
    "    print(f\"Task B: RFQ Similarity Analysis - COMPLETED\")\n",
    "    print(f\"=\" * 50)\n",
    "    \n",
    "    # Data processing summary\n",
    "    print(f\"\\n📊 DATA PROCESSING SUMMARY:\")\n",
    "    print(f\"   • Original RFQ records: 1,000\")\n",
    "    print(f\"   • RFQs with reference data: {enriched_df['Grade/Material'].notna().sum()}\")\n",
    "    print(f\"   • Grade properties parsed: 18 properties\")\n",
    "    print(f\"   • Feature engineering: 157 total features\")\n",
    "    \n",
    "    # Similarity analysis summary  \n",
    "    print(f\"\\n🔍 SIMILARITY ANALYSIS SUMMARY:\")\n",
    "    print(f\"   • Total similarity pairs calculated: {len(similarity_results):,}\")\n",
    "    print(f\"   • Average similarity score: {similarity_results['similarity_score'].mean():.3f}\")\n",
    "    print(f\"   • Similarity score range: {similarity_results['similarity_score'].min():.3f} - {similarity_results['similarity_score'].max():.3f}\")\n",
    "    \n",
    "    # Component analysis\n",
    "    print(f\"\\n⚖️ SIMILARITY COMPONENTS:\")\n",
    "    print(f\"   • Dimension similarity avg: {similarity_results['dimension_similarity'].mean():.3f}\")\n",
    "    print(f\"   • Categorical similarity avg: {similarity_results['categorical_similarity'].mean():.3f}\")\n",
    "    print(f\"   • Property similarity avg: {similarity_results['property_similarity'].mean():.3f}\")\n",
    "    \n",
    "    # Top performing pairs\n",
    "    print(f\"\\n🏆 TOP SIMILARITY PAIRS:\")\n",
    "    top5 = similarity_results.nlargest(5, 'similarity_score')\n",
    "    for idx, row in top5.iterrows():\n",
    "        print(f\"   • {row['rfq_id'][:8]}... ↔ {row['match_id'][:8]}... (score: {row['similarity_score']:.3f})\")\n",
    "\n",
    "# Execute final deliverable creation\n",
    "final_deliverable = create_final_deliverable(similarity_results)\n",
    "\n",
    "# Create summary report\n",
    "create_summary_report(similarity_results, feature_engineered_df)\n",
    "\n",
    "print(f\"\\n🎉 TASK B COMPLETED SUCCESSFULLY!\")\n",
    "print(f\"   All required deliverables have been generated.\")\n",
    "print(f\"   The similarity pipeline is ready for production use.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "15f80296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Data Loading ===\n",
      "\n",
      "=== TASK B.1: Grade Normalization and Reference Join ===\n",
      "\n",
      "Grade Analysis:\n",
      "Unique normalized grades in RFQ: 147\n",
      "Unique normalized grades in Reference: 164\n",
      "Grades found in both datasets: 147\n",
      "RFQ grades missing in reference: 0\n",
      "Reference grades not in RFQ: 17\n",
      "\n",
      "Join Results:\n",
      "Total RFQ records: 1000\n",
      "Records with reference data: 941\n",
      "Records missing reference data: 59\n",
      "Enriched RFQ shape: (1000, 61)\n",
      "No duplicate rows created!\n",
      "\n",
      "=== TASK B.1: Range String Parsing ===\n",
      "Parsing Carbon (C)...\n",
      "  Successfully parsed 940/941 values\n",
      "Parsing Manganese (Mn)...\n",
      "  Successfully parsed 940/941 values\n",
      "Parsing Silicon (Si)...\n",
      "  Successfully parsed 539/540 values\n",
      "Parsing Sulfur (S)...\n",
      "  Successfully parsed 929/929 values\n",
      "Parsing Phosphorus (P)...\n",
      "  Successfully parsed 929/929 values\n",
      "Parsing Chromium (Cr)...\n",
      "  Successfully parsed 63/63 values\n",
      "Parsing Nickel (Ni)...\n",
      "  Successfully parsed 22/22 values\n",
      "Parsing Molybdenum (Mo)...\n",
      "  Successfully parsed 41/41 values\n",
      "Parsing Vanadium (V)...\n",
      "  Successfully parsed 227/227 values\n",
      "Parsing Copper (Cu)...\n",
      "  Successfully parsed 0/0 values\n",
      "Parsing Aluminum (Al)...\n",
      "  Successfully parsed 136/540 values\n",
      "Parsing Titanium (Ti)...\n",
      "  Successfully parsed 230/230 values\n",
      "Parsing Niobium (Nb)...\n",
      "  Successfully parsed 229/229 values\n",
      "Parsing Boron (B)...\n",
      "  Successfully parsed 7/7 values\n",
      "Parsing Nitrogen (N)...\n",
      "  Successfully parsed 136/136 values\n",
      "Parsing Tensile strength (Rm)...\n",
      "  Successfully parsed 940/941 values\n",
      "Parsing Yield strength (Re or Rp0.2)...\n",
      "  Successfully parsed 350/930 values\n",
      "Parsing Elongation (A%)...\n",
      "  Successfully parsed 2/921 values\n",
      "Total properties parsed: 18\n",
      "Final dataset shape: (1000, 115)\n",
      "Existing categorical features: ['coating', 'finish', 'form', 'surface_type', 'surface_protection']\n",
      "Total found: 5\n"
     ]
    }
   ],
   "source": [
    "# Quick check - run this in your scripts/Task_B/ directory\n",
    "from data_loader import load_rfq_data\n",
    "from grade_normalizer import normalize_grades\n",
    "from range_parser import parse_range_strings\n",
    "\n",
    "rfq_df, reference_df = load_rfq_data()\n",
    "enriched_df, _ = normalize_grades(rfq_df, reference_df)\n",
    "parsed_df = parse_range_strings(enriched_df)\n",
    "\n",
    "categorical_features = ['coating', 'finish', 'form', 'surface_type', 'surface_protection']\n",
    "existing_features = [f for f in categorical_features if f in parsed_df.columns]\n",
    "print('Existing categorical features:', existing_features)\n",
    "print('Total found:', len(existing_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d370b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
